{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ideal-turkish",
   "metadata": {},
   "source": [
    "# Text Analysis for Women's E-Commerce Clothing Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-composite",
   "metadata": {},
   "source": [
    "## Libraries and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Feature engineering\n",
    "import string\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-standing",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DIR = os.path.join(os.pardir, 'data', 'processed')\n",
    "RANDOM_STATE = 8\n",
    "VARIABLES_DROP = ['review_text', 'review_lower',\n",
    "       'review_nopct', 'review_nodg', 'review_word_tokens', 'review_no_sw',\n",
    "       'review_stem', 'review_lem']\n",
    "TARGET = 'recommended_ind'\n",
    "FEATURED_DIR = os.path.join(os.pardir, 'data', 'featured')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-shannon",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, filename):\n",
    "    data_path = os.path.join(path, filename)\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df\n",
    "\n",
    "def get_count_words(s):\n",
    "    return len(str(s).split(\" \"))\n",
    "\n",
    "def get_count_char(s):\n",
    "    return sum(len(w) for w in str(s).split(\" \"))\n",
    "\n",
    "def get_count_sents(s):\n",
    "    return len(str(s).split(\".\"))\n",
    "\n",
    "def get_count_exc_marks(s):\n",
    "    return s.count('!')\n",
    "\n",
    "def get_count_question_marks(s):\n",
    "    return s.count('?')\n",
    "    \n",
    "def get_count_pct(s):\n",
    "    return len([w for w in s if w in '\"#$%&\\'()*+,-./:;<=>@[\\\\]^_`{|}~'])\n",
    "\n",
    "def get_count_cap(s):\n",
    "    return sum(1 for w in s if w.isupper())\n",
    "\n",
    "def get_polarity(s):\n",
    "    tb = TextBlob(s)\n",
    "    return tb.sentiment.polarity\n",
    "\n",
    "def get_subjectivity(s):\n",
    "    tb = TextBlob(s)\n",
    "    return tb.sentiment.subjectivity\n",
    "\n",
    "def get_text_features(df):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # word count\n",
    "    df_copy['word_count'] = df_copy['review_text'].apply(get_count_words)\n",
    "    # character count\n",
    "    df_copy['char_count'] = df_copy['review_text'].apply(get_count_char)\n",
    "    # sentence count\n",
    "    df_copy['sentence_count'] = df_copy['review_text'].apply(get_count_sents)\n",
    "    # count capitals\n",
    "    df_copy['capitals_count'] = df_copy['review_text'].apply(get_count_cap)\n",
    "    # count puncts\n",
    "    df_copy['punc_count'] = df_copy['review_text'].apply(get_count_pct)\n",
    "    df_copy['exc_marks_count'] = df_copy['review_text'].apply(get_count_exc_marks)\n",
    "    df_copy['question_marks_count'] = df_copy['review_text'].apply(get_count_question_marks)\n",
    "    # avg word len\n",
    "    df_copy['avg_word_len'] = df_copy['char_count'] / df_copy['word_count']\n",
    "    # avg sentence len\n",
    "    df_copy['avg_sentence_len'] = df_copy['word_count'] / df_copy['sentence_count']\n",
    "    # avg cap\n",
    "    df_copy['avg_cap_len']= df_copy.apply(lambda row: float(row['capitals_count'])/float(row['word_count']), axis=1)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def get_nlp_features(df):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # polarity\n",
    "    df_copy['polarity'] = df_copy['review_text'].apply(get_polarity)\n",
    "    # subjectivity\n",
    "    df_copy['subjectivity'] = df_copy['review_text'].apply(get_subjectivity)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def get_abt_df(df, tfidf, features, target, drop_cols):\n",
    "    df = df.copy()\n",
    "    tfidf_plain = tfidf.toarray()\n",
    "    tfidf_df = pd.DataFrame(tfidf_plain, columns=features)\n",
    "    df = df.drop(columns=drop_cols)\n",
    "    abt_df = pd.merge(df, tfidf_df, left_index=True, right_index=True)\n",
    "    cols = [col for col in abt_df if col != target] + [target]\n",
    "    abt_df = abt_df[cols]\n",
    "    return abt_df\n",
    "\n",
    "def save_data(df, path, filename):\n",
    "    data_path = os.path.join(path, filename)\n",
    "    df.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-audience",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_data(PROCESSED_DIR, 'train.csv')\n",
    "test = load_data(PROCESSED_DIR, 'test.csv')\n",
    "val = load_data(PROCESSED_DIR, 'val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-ground",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-elimination",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-rates",
   "metadata": {},
   "source": [
    "### Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_feats = get_text_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_feats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_feats = get_text_features(test)\n",
    "val_text_feats = get_text_features(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-mileage",
   "metadata": {},
   "source": [
    "### More NLP based features \n",
    "\n",
    "**TODO: Add Part to Speech**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nlp_feats = get_nlp_features(train_text_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nlp_feats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-compilation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_nlp_feats = get_nlp_features(test_text_feats)\n",
    "val_nlp_feats = get_nlp_features(val_text_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-carrier",
   "metadata": {},
   "source": [
    "### TF-IDF Feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "tf_idf_vectorizer = tf_idf_vectorizer.fit(train_nlp_feats['review_lem'])\n",
    "tf_idf_train_matrix = tf_idf_vectorizer.transform(train_nlp_feats['review_lem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_test_matrix = tf_idf_vectorizer.transform(test_nlp_feats['review_lem'])\n",
    "tf_idf_val_matrix = tf_idf_vectorizer.transform(val_nlp_feats['review_lem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-stream",
   "metadata": {},
   "source": [
    "### TODO: Add Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-estate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "blind-african",
   "metadata": {},
   "source": [
    "### TODO: Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-warrant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "literary-tenant",
   "metadata": {},
   "source": [
    "## ABTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = tf_idf_vectorizer.get_feature_names()\n",
    "abt_train = get_abt_df(train_nlp_feats, tf_idf_train_matrix, FEATURES, TARGET, VARIABLES_DROP)\n",
    "abt_test = get_abt_df(test_nlp_feats, tf_idf_test_matrix, FEATURES, TARGET, VARIABLES_DROP)\n",
    "abt_val = get_abt_df(val_nlp_feats, tf_idf_val_matrix, FEATURES, TARGET, VARIABLES_DROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "abt_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "abt_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "abt_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-worthy",
   "metadata": {},
   "source": [
    "## Store Featured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "abts = [abt_train, abt_test, abt_val]\n",
    "fnames = ['train.csv', 'test.csv', 'val.csv']\n",
    "\n",
    "p = Path(FEATURED_DIR)\n",
    "if not p.exists():\n",
    "    os.mkdir(p)\n",
    "for df, fname in zip(abts, fnames):\n",
    "    save_data(df=df, path=FEATURED_DIR, filename=fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-tribute",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

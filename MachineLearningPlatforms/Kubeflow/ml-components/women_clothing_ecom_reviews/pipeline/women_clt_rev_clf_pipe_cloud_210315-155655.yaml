apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: women-clothing-reviews-classification-ml-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.4.0, pipelines.kubeflow.org/pipeline_compilation_time: '2021-03-15T15:56:55.781671',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "An example of Machine
      Learning Pipeline", "inputs": [{"name": "mode"}, {"name": "bucket"}, {"name":
      "config_file"}], "name": "Women Clothing Reviews Classification ML Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.4.0}
spec:
  entrypoint: women-clothing-reviews-classification-ml-pipeline
  templates:
  - name: read-pipeline-configuration
    container:
      args: []
      command:
      - bash
      - -ex
      - -c
      - |
        if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
            gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
        fi

        uri="$0"
        output_path="$1"

        # Checking whether the URI points to a single blob, a directory or a URI pattern
        # URI points to a blob when that URI does not end with slash and listing that URI only yields the same URI
        if [[ "$uri" != */ ]] && (gsutil ls "$uri" | grep --fixed-strings --line-regexp "$uri"); then
            mkdir -p "$(dirname "$output_path")"
            gsutil -m cp -r "$uri" "$output_path"
        else
            mkdir -p "$output_path" # When source path is a directory, gsutil requires the destination to also be a directory
            gsutil -m rsync -r "$uri" "$output_path" # gsutil cp has different path handling than Linux cp. It always puts the source directory (name) inside the destination directory. gsutil rsync does not have that problem.
        fi
      - '{{inputs.parameters.bucket}}/{{inputs.parameters.config_file}}'
      - /tmp/outputs/Data/data
      image: google/cloud-sdk
    inputs:
      parameters:
      - {name: bucket}
      - {name: config_file}
    outputs:
      parameters:
      - name: read-pipeline-configuration-Data
        valueFrom: {path: /tmp/outputs/Data/data}
      artifacts:
      - {name: read-pipeline-configuration-Data, path: /tmp/outputs/Data/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"command": ["bash", "-ex", "-c", "if [ -n \"${GOOGLE_APPLICATION_CREDENTIALS}\"
          ]; then\n    gcloud auth activate-service-account --key-file=\"${GOOGLE_APPLICATION_CREDENTIALS}\"\nfi\n\nuri=\"$0\"\noutput_path=\"$1\"\n\n#
          Checking whether the URI points to a single blob, a directory or a URI pattern\n#
          URI points to a blob when that URI does not end with slash and listing that
          URI only yields the same URI\nif [[ \"$uri\" != */ ]] && (gsutil ls \"$uri\"
          | grep --fixed-strings --line-regexp \"$uri\"); then\n    mkdir -p \"$(dirname
          \"$output_path\")\"\n    gsutil -m cp -r \"$uri\" \"$output_path\"\nelse\n    mkdir
          -p \"$output_path\" # When source path is a directory, gsutil requires the
          destination to also be a directory\n    gsutil -m rsync -r \"$uri\" \"$output_path\"
          # gsutil cp has different path handling than Linux cp. It always puts the
          source directory (name) inside the destination directory. gsutil rsync does
          not have that problem.\nfi\n", {"inputValue": "GCS path"}, {"outputPath":
          "Data"}], "image": "google/cloud-sdk"}}, "inputs": [{"name": "GCS path",
          "type": "URI"}], "name": "Read Pipeline configuration", "outputs": [{"name":
          "Data"}]}', pipelines.kubeflow.org/component_ref: '{"digest": "1fbe46e98817e5bc74b7c1e3ebe57b5dd57403d74b5123170aa270b95a0471ef",
          "url": "components/config_init/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"GCS
          path": "{{inputs.parameters.bucket}}/{{inputs.parameters.config_file}}"}'}
  - name: run-collect
    container:
      args: [--mode, '{{inputs.parameters.mode}}', --bucket, '{{inputs.parameters.bucket}}',
        --config, '{{inputs.parameters.read-pipeline-configuration-Data}}', '----output-paths',
        /tmp/outputs/train/data, /tmp/outputs/test/data, /tmp/outputs/val/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def run_collect(mode,
                        bucket,
                        config):

            # Libraries --------------------------------------------------------------------------------------------------------

            import logging
            import yaml
            from collections import namedtuple
            import sys
            from collect import DataCollector

            # Settings ---------------------------------------------------------------------------------------------------------
            logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',
                                datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)

            logging.info('Initializing pipeline configuration...')

            try:
                # TODO: Check for one to one portability with cloud
                if mode == 'cloud':
                    config = yaml.safe_load(config)
                else:
                    stream = open(config, 'r')
                    config = yaml.load(stream=stream, Loader=yaml.FullLoader)

                collector = DataCollector(config=config)
                raw_df = collector.extract()
                # TODO: Add metadata in the pipeline
                print(raw_df.head(5))
                x_train, x_test, x_val, y_train, y_test, y_val = collector.transform(raw_df)

                if mode == 'cloud':
                    (train_path_gcs, test_path_gcs, val_path_gcs) = collector.load(x_train, x_test, x_val,
                                                                                   y_train, y_test, y_val, mode=mode,
                                                                                   bucket=bucket)
                    out_gcs = namedtuple('output_paths', ['train', 'test', 'val'])
                    return out_gcs(train_path_gcs, test_path_gcs, val_path_gcs)
                else:
                    (train_path, test_path, val_path) = collector.load(x_train, x_test, x_val,
                                                                       y_train, y_test, y_val, mode=mode, bucket=bucket)
                    out_path = namedtuple('output_paths', ['train', 'test', 'val'])
                    return out_path(train_path, test_path, val_path)
            except RuntimeError as error:
                logging.info(error)
                sys.exit(1)

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Run collect', description='')
        _parser.add_argument("--mode", dest="mode", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--bucket", dest="bucket", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--config", dest="config", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = run_collect(**_parsed_args)

        _output_serializers = [
            _serialize_str,
            _serialize_str,
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      env:
      - {name: GOOGLE_APPLICATION_CREDENTIALS, value: /secret/gcp-credentials/user-gcp-sa.json}
      - {name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE, value: /secret/gcp-credentials/user-gcp-sa.json}
      image: docker.io/in92/data_collect:1.0.3
      volumeMounts:
      - {mountPath: /secret/gcp-credentials, name: gcp-credentials-user-gcp-sa}
    inputs:
      parameters:
      - {name: bucket}
      - {name: mode}
      - {name: read-pipeline-configuration-Data}
    outputs:
      artifacts:
      - {name: run-collect-test, path: /tmp/outputs/test/data}
      - {name: run-collect-train, path: /tmp/outputs/train/data}
      - {name: run-collect-val, path: /tmp/outputs/val/data}
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret: {secretName: user-gcp-sa}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--mode", {"inputValue": "mode"}, "--bucket", {"inputValue": "bucket"},
          "--config", {"inputValue": "config"}, "----output-paths", {"outputPath":
          "train"}, {"outputPath": "test"}, {"outputPath": "val"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def run_collect(mode,\n                bucket,\n                config):\n\n    #
          Libraries --------------------------------------------------------------------------------------------------------\n\n    import
          logging\n    import yaml\n    from collections import namedtuple\n    import
          sys\n    from collect import DataCollector\n\n    # Settings ---------------------------------------------------------------------------------------------------------\n    logging.basicConfig(format=''%(asctime)s
          %(levelname)s %(message)s'',\n                        datefmt=''%m/%d/%Y
          %I:%M:%S %p'', level=logging.INFO)\n\n    logging.info(''Initializing pipeline
          configuration...'')\n\n    try:\n        # TODO: Check for one to one portability
          with cloud\n        if mode == ''cloud'':\n            config = yaml.safe_load(config)\n        else:\n            stream
          = open(config, ''r'')\n            config = yaml.load(stream=stream, Loader=yaml.FullLoader)\n\n        collector
          = DataCollector(config=config)\n        raw_df = collector.extract()\n        #
          TODO: Add metadata in the pipeline\n        print(raw_df.head(5))\n        x_train,
          x_test, x_val, y_train, y_test, y_val = collector.transform(raw_df)\n\n        if
          mode == ''cloud'':\n            (train_path_gcs, test_path_gcs, val_path_gcs)
          = collector.load(x_train, x_test, x_val,\n                                                                           y_train,
          y_test, y_val, mode=mode,\n                                                                           bucket=bucket)\n            out_gcs
          = namedtuple(''output_paths'', [''train'', ''test'', ''val''])\n            return
          out_gcs(train_path_gcs, test_path_gcs, val_path_gcs)\n        else:\n            (train_path,
          test_path, val_path) = collector.load(x_train, x_test, x_val,\n                                                               y_train,
          y_test, y_val, mode=mode, bucket=bucket)\n            out_path = namedtuple(''output_paths'',
          [''train'', ''test'', ''val''])\n            return out_path(train_path,
          test_path, val_path)\n    except RuntimeError as error:\n        logging.info(error)\n        sys.exit(1)\n\ndef
          _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(str(str_value), str(type(str_value))))\n    return str_value\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Run collect'', description='''')\n_parser.add_argument(\"--mode\",
          dest=\"mode\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--bucket\",
          dest=\"bucket\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--config\",
          dest=\"config\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = run_collect(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n    _serialize_str,\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.io/in92/data_collect:1.0.3"}}, "inputs": [{"name": "mode",
          "type": "String"}, {"name": "bucket", "type": "String"}, {"name": "config",
          "type": "String"}], "name": "Run collect", "outputs": [{"name": "train",
          "type": "String"}, {"name": "test", "type": "String"}, {"name": "val", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest": "fd8b7908cdc56d397c91039aaf3e2648c216bb5bfb090ce590d81404a30f6039",
          "url": "components/data_collection/women_clt_rev_clf_cloud_data_collection_component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"bucket": "{{inputs.parameters.bucket}}",
          "config": "{{inputs.parameters.read-pipeline-configuration-Data}}", "mode":
          "{{inputs.parameters.mode}}"}'}
  - name: women-clothing-reviews-classification-ml-pipeline
    inputs:
      parameters:
      - {name: bucket}
      - {name: config_file}
      - {name: mode}
    dag:
      tasks:
      - name: read-pipeline-configuration
        template: read-pipeline-configuration
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: config_file, value: '{{inputs.parameters.config_file}}'}
      - name: run-collect
        template: run-collect
        dependencies: [read-pipeline-configuration]
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: mode, value: '{{inputs.parameters.mode}}'}
          - {name: read-pipeline-configuration-Data, value: '{{tasks.read-pipeline-configuration.outputs.parameters.read-pipeline-configuration-Data}}'}
  arguments:
    parameters:
    - {name: mode}
    - {name: bucket}
    - {name: config_file}
  serviceAccountName: pipeline-runner

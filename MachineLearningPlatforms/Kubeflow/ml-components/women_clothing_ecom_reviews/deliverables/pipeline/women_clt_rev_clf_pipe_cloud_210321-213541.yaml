apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: women-clothing-reviews-classification-ml-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.4.0, pipelines.kubeflow.org/pipeline_compilation_time: '2021-03-21T21:35:41.522984',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "An example of Machine
      Learning Pipeline", "inputs": [{"default": "config.yaml", "name": "config_file",
      "optional": true}, {"name": "mode", "optional": true}, {"name": "bucket", "optional":
      true}], "name": "Women Clothing Reviews Classification ML Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.4.0}
spec:
  entrypoint: women-clothing-reviews-classification-ml-pipeline
  templates:
  - name: collect-data
    container:
      args: [--config, '{{inputs.parameters.config_file}}', --mode, '{{inputs.parameters.mode}}',
        --bucket, '{{inputs.parameters.bucket}}', '----output-paths', /tmp/outputs/train/data,
        /tmp/outputs/test/data, /tmp/outputs/val/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def run_collect(config,
                        mode,
                        bucket):
            # Libraries --------------------------------------------------------------------------------------------------------
            import logging
            import yaml
            from collections import namedtuple
            import sys
            from src.collect import DataCollector

            # Settings ---------------------------------------------------------------------------------------------------------
            logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',
                                datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)

            logging.info('Initializing pipeline configuration...')

            try:
                stream = open(config, 'r')
                config = yaml.load(stream=stream, Loader=yaml.FullLoader)
                collector = DataCollector(config=config)
                raw_df = collector.extract(mode=mode, bucket=bucket)
                # TODO: Add metadata in the pipeline
                print(raw_df.head(5))
                x_train, x_test, x_val, y_train, y_test, y_val = collector.transform(raw_df)

                if mode == 'cloud':
                    (train_path_gcs, test_path_gcs, val_path_gcs) = collector.load(x_train, x_test, x_val,
                                                                                   y_train, y_test, y_val, mode=mode,
                                                                                   bucket=bucket)
                    out_gcs = namedtuple('output_paths', ['train', 'test', 'val'])
                    return out_gcs(train_path_gcs, test_path_gcs, val_path_gcs)
                else:
                    (train_path, test_path, val_path) = collector.load(x_train, x_test, x_val,
                                                                       y_train, y_test, y_val, mode=mode, bucket=bucket)
                    out_path = namedtuple('output_paths', ['train', 'test', 'val'])
                    return out_path(train_path, test_path, val_path)
            except RuntimeError as error:
                logging.info(error)
                sys.exit(1)

        import argparse
        _parser = argparse.ArgumentParser(prog='Run collect', description='')
        _parser.add_argument("--config", dest="config", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mode", dest="mode", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--bucket", dest="bucket", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = run_collect(**_parsed_args)

        _output_serializers = [
            str,
            str,
            str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      env:
      - {name: GOOGLE_APPLICATION_CREDENTIALS, value: /secret/gcp-credentials/user-gcp-sa.json}
      - {name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE, value: /secret/gcp-credentials/user-gcp-sa.json}
      image: docker.io/in92/data_collect:1.0.1
      volumeMounts:
      - {mountPath: /secret/gcp-credentials, name: gcp-credentials-user-gcp-sa}
    inputs:
      parameters:
      - {name: bucket}
      - {name: config_file}
      - {name: mode}
    outputs:
      parameters:
      - name: collect-data-test
        valueFrom: {path: /tmp/outputs/test/data}
      - name: collect-data-train
        valueFrom: {path: /tmp/outputs/train/data}
      - name: collect-data-val
        valueFrom: {path: /tmp/outputs/val/data}
      artifacts:
      - {name: collect-data-test, path: /tmp/outputs/test/data}
      - {name: collect-data-train, path: /tmp/outputs/train/data}
      - {name: collect-data-val, path: /tmp/outputs/val/data}
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret: {secretName: user-gcp-sa}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--config", {"inputValue": "config"}, "--mode", {"inputValue":
          "mode"}, "--bucket", {"inputValue": "bucket"}, "----output-paths", {"outputPath":
          "train"}, {"outputPath": "test"}, {"outputPath": "val"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def run_collect(config,\n                mode,\n                bucket):\n    #
          Libraries --------------------------------------------------------------------------------------------------------\n    import
          logging\n    import yaml\n    from collections import namedtuple\n    import
          sys\n    from src.collect import DataCollector\n\n    # Settings ---------------------------------------------------------------------------------------------------------\n    logging.basicConfig(format=''%(asctime)s
          %(levelname)s %(message)s'',\n                        datefmt=''%m/%d/%Y
          %I:%M:%S %p'', level=logging.INFO)\n\n    logging.info(''Initializing pipeline
          configuration...'')\n\n    try:\n        stream = open(config, ''r'')\n        config
          = yaml.load(stream=stream, Loader=yaml.FullLoader)\n        collector =
          DataCollector(config=config)\n        raw_df = collector.extract(mode=mode,
          bucket=bucket)\n        # TODO: Add metadata in the pipeline\n        print(raw_df.head(5))\n        x_train,
          x_test, x_val, y_train, y_test, y_val = collector.transform(raw_df)\n\n        if
          mode == ''cloud'':\n            (train_path_gcs, test_path_gcs, val_path_gcs)
          = collector.load(x_train, x_test, x_val,\n                                                                           y_train,
          y_test, y_val, mode=mode,\n                                                                           bucket=bucket)\n            out_gcs
          = namedtuple(''output_paths'', [''train'', ''test'', ''val''])\n            return
          out_gcs(train_path_gcs, test_path_gcs, val_path_gcs)\n        else:\n            (train_path,
          test_path, val_path) = collector.load(x_train, x_test, x_val,\n                                                               y_train,
          y_test, y_val, mode=mode, bucket=bucket)\n            out_path = namedtuple(''output_paths'',
          [''train'', ''test'', ''val''])\n            return out_path(train_path,
          test_path, val_path)\n    except RuntimeError as error:\n        logging.info(error)\n        sys.exit(1)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Run collect'', description='''')\n_parser.add_argument(\"--config\",
          dest=\"config\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mode\",
          dest=\"mode\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--bucket\",
          dest=\"bucket\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = run_collect(**_parsed_args)\n\n_output_serializers
          = [\n    str,\n    str,\n    str,\n\n]\n\nimport os\nfor idx, output_file
          in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.io/in92/data_collect:1.0.1"}}, "inputs": [{"name": "config",
          "type": "String"}, {"name": "mode", "type": "String"}, {"name": "bucket",
          "type": "String"}], "name": "Collect Data", "outputs": [{"name": "train",
          "type": "GCSPath"}, {"name": "test", "type": "GCSPath"}, {"name": "val",
          "type": "GCSPath"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "e2a8ee0bd56c06b08044c2138c53d890fe84b6a00f49650b8488d8321758bf83", "url":
          "../deliverables/components/data_collection/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"bucket":
          "{{inputs.parameters.bucket}}", "config": "{{inputs.parameters.config_file}}",
          "mode": "{{inputs.parameters.mode}}"}'}
  - name: run-generate-features
    container:
      args: [--config, '{{inputs.parameters.config_file}}', --mode, '{{inputs.parameters.mode}}',
        --bucket, '{{inputs.parameters.bucket}}', --train-path, '{{inputs.parameters.run-prepare-train}}',
        --test-path, '{{inputs.parameters.run-prepare-test}}', --val-path, '{{inputs.parameters.run-prepare-val}}',
        '----output-paths', /tmp/outputs/train/data, /tmp/outputs/test/data, /tmp/outputs/val/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def run_generate_features(config,
                        mode,
                        bucket,
                        train_path,
                        test_path,
                        val_path):
            # Libraries --------------------------------------------------------------------------------------------------------
            import logging.config
            import yaml
            import sys
            import os
            from src.generate_features import FeaturesGenerator
            from src.helpers import load_data, get_abt_df, save_data

            # Settings ---------------------------------------------------------------------------------------------------------
            logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',
                                datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)

            try:
                logging.info('Initializing configuration...')
                stream = open(config, 'r')
                config = yaml.load(stream=stream, Loader=yaml.FullLoader)
                feats_generator = FeaturesGenerator(config=config)

                if mode == 'cloud':
                    output_paths_gcs = []

                    # Train - Inference
                    train_data = load_data(input_data=train_path, mode=mode)
                    feats_generator.fit(data=train_data[config['lem_variable']])
                    features = feats_generator.tf_idf_vectorizer.get_feature_names()
                    train_tf_idf_matrix = feats_generator.transform(data=train_data)
                    train_abt = get_abt_df(df=train_data, tfidf=train_tf_idf_matrix,
                                           features=features, target=config['target'], drop_cols=config['variables_to_drop'])
                    train_path_gcs = save_data(df=train_abt, path=config['featured_path'],
                                                 out_data=config['featured_data'][0], mode=mode, bucket=bucket)
                    output_paths_gcs.append(train_path_gcs)

                    # Test, Val - Prediction
                    for input_path, out_filename in zip([test_path, val_path], config['featured_data']):
                        data = load_data(input_data=input_path, mode=mode)
                        tf_idf_matrix = feats_generator.transform(data=data)
                        abt = get_abt_df(df=data, tfidf=tf_idf_matrix,
                                           features=features, target=config['target'], drop_cols=config['variables_to_drop'])
                        out_path_gcs = save_data(df=abt, path=config['featured_path'],
                                                   out_data=out_filename, mode=mode, bucket=bucket)
                        output_paths_gcs.append(out_path_gcs)
                    return tuple(output_paths_gcs)
                else:
                    output_paths = []

                    # Train - Inference
                    train_data_path = os.path.join(config['processed_path'], config['processed_data'][0])
                    train_data = load_data(input_data=train_data_path, mode=mode)
                    feats_generator.fit(data=train_data[config['lem_variable']])
                    features = feats_generator.tf_idf_vectorizer.get_feature_names()
                    train_tf_idf_matrix = feats_generator.transform(data=train_data)
                    train_abt = get_abt_df(df=train_data, tfidf=train_tf_idf_matrix,
                                           features=features, target=config['target'], drop_cols=config['variables_to_drop'])
                    train_path = save_data(df=train_abt, path=config['featured_path'],
                                               out_data=config['featured_data'][0], mode=mode, bucket=bucket)
                    output_paths.append(train_path)

                    # Test, Val - Prediction
                    for input_filename, out_filename in zip(config['processed_data'][1:], config['featured_data']):
                        data_path = os.path.join(config['processed_path'], input_filename)
                        data = load_data(input_data=data_path, mode=mode)
                        tf_idf_matrix = feats_generator.transform(data=data)
                        abt = get_abt_df(df=data, tfidf=tf_idf_matrix,
                                         features=features, target=config['target'], drop_cols=config['variables_to_drop'])
                        out_path = save_data(df=abt, path=config['featured_path'],
                                                 out_data=out_filename, mode=mode, bucket=bucket)
                        output_paths.append(out_path)
                    return tuple(output_paths)

            except RuntimeError as error:
                logging.info(error)
                sys.exit(1)

        import argparse
        _parser = argparse.ArgumentParser(prog='Run generate features', description='')
        _parser.add_argument("--config", dest="config", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mode", dest="mode", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--bucket", dest="bucket", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train-path", dest="train_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-path", dest="test_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--val-path", dest="val_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = run_generate_features(**_parsed_args)

        _output_serializers = [
            str,
            str,
            str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      env:
      - {name: GOOGLE_APPLICATION_CREDENTIALS, value: /secret/gcp-credentials/user-gcp-sa.json}
      - {name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE, value: /secret/gcp-credentials/user-gcp-sa.json}
      image: docker.io/in92/feature_generator:1.0.1
      volumeMounts:
      - {mountPath: /secret/gcp-credentials, name: gcp-credentials-user-gcp-sa}
    inputs:
      parameters:
      - {name: bucket}
      - {name: config_file}
      - {name: mode}
      - {name: run-prepare-test}
      - {name: run-prepare-train}
      - {name: run-prepare-val}
    outputs:
      artifacts:
      - {name: run-generate-features-test, path: /tmp/outputs/test/data}
      - {name: run-generate-features-train, path: /tmp/outputs/train/data}
      - {name: run-generate-features-val, path: /tmp/outputs/val/data}
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret: {secretName: user-gcp-sa}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--config", {"inputValue": "config"}, "--mode", {"inputValue":
          "mode"}, "--bucket", {"inputValue": "bucket"}, "--train-path", {"inputValue":
          "train_path"}, "--test-path", {"inputValue": "test_path"}, "--val-path",
          {"inputValue": "val_path"}, "----output-paths", {"outputPath": "train"},
          {"outputPath": "test"}, {"outputPath": "val"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def run_generate_features(config,\n                mode,\n                bucket,\n                train_path,\n                test_path,\n                val_path):\n    #
          Libraries --------------------------------------------------------------------------------------------------------\n    import
          logging.config\n    import yaml\n    import sys\n    import os\n    from
          src.generate_features import FeaturesGenerator\n    from src.helpers import
          load_data, get_abt_df, save_data\n\n    # Settings ---------------------------------------------------------------------------------------------------------\n    logging.basicConfig(format=''%(asctime)s
          %(levelname)s %(message)s'',\n                        datefmt=''%m/%d/%Y
          %I:%M:%S %p'', level=logging.INFO)\n\n    try:\n        logging.info(''Initializing
          configuration...'')\n        stream = open(config, ''r'')\n        config
          = yaml.load(stream=stream, Loader=yaml.FullLoader)\n        feats_generator
          = FeaturesGenerator(config=config)\n\n        if mode == ''cloud'':\n            output_paths_gcs
          = []\n\n            # Train - Inference\n            train_data = load_data(input_data=train_path,
          mode=mode)\n            feats_generator.fit(data=train_data[config[''lem_variable'']])\n            features
          = feats_generator.tf_idf_vectorizer.get_feature_names()\n            train_tf_idf_matrix
          = feats_generator.transform(data=train_data)\n            train_abt = get_abt_df(df=train_data,
          tfidf=train_tf_idf_matrix,\n                                   features=features,
          target=config[''target''], drop_cols=config[''variables_to_drop''])\n            train_path_gcs
          = save_data(df=train_abt, path=config[''featured_path''],\n                                         out_data=config[''featured_data''][0],
          mode=mode, bucket=bucket)\n            output_paths_gcs.append(train_path_gcs)\n\n            #
          Test, Val - Prediction\n            for input_path, out_filename in zip([test_path,
          val_path], config[''featured_data'']):\n                data = load_data(input_data=input_path,
          mode=mode)\n                tf_idf_matrix = feats_generator.transform(data=data)\n                abt
          = get_abt_df(df=data, tfidf=tf_idf_matrix,\n                                   features=features,
          target=config[''target''], drop_cols=config[''variables_to_drop''])\n                out_path_gcs
          = save_data(df=abt, path=config[''featured_path''],\n                                           out_data=out_filename,
          mode=mode, bucket=bucket)\n                output_paths_gcs.append(out_path_gcs)\n            return
          tuple(output_paths_gcs)\n        else:\n            output_paths = []\n\n            #
          Train - Inference\n            train_data_path = os.path.join(config[''processed_path''],
          config[''processed_data''][0])\n            train_data = load_data(input_data=train_data_path,
          mode=mode)\n            feats_generator.fit(data=train_data[config[''lem_variable'']])\n            features
          = feats_generator.tf_idf_vectorizer.get_feature_names()\n            train_tf_idf_matrix
          = feats_generator.transform(data=train_data)\n            train_abt = get_abt_df(df=train_data,
          tfidf=train_tf_idf_matrix,\n                                   features=features,
          target=config[''target''], drop_cols=config[''variables_to_drop''])\n            train_path
          = save_data(df=train_abt, path=config[''featured_path''],\n                                       out_data=config[''featured_data''][0],
          mode=mode, bucket=bucket)\n            output_paths.append(train_path)\n\n            #
          Test, Val - Prediction\n            for input_filename, out_filename in
          zip(config[''processed_data''][1:], config[''featured_data'']):\n                data_path
          = os.path.join(config[''processed_path''], input_filename)\n                data
          = load_data(input_data=data_path, mode=mode)\n                tf_idf_matrix
          = feats_generator.transform(data=data)\n                abt = get_abt_df(df=data,
          tfidf=tf_idf_matrix,\n                                 features=features,
          target=config[''target''], drop_cols=config[''variables_to_drop''])\n                out_path
          = save_data(df=abt, path=config[''featured_path''],\n                                         out_data=out_filename,
          mode=mode, bucket=bucket)\n                output_paths.append(out_path)\n            return
          tuple(output_paths)\n\n    except RuntimeError as error:\n        logging.info(error)\n        sys.exit(1)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Run generate features'',
          description='''')\n_parser.add_argument(\"--config\", dest=\"config\", type=str,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mode\",
          dest=\"mode\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--bucket\",
          dest=\"bucket\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-path\",
          dest=\"train_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-path\",
          dest=\"test_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--val-path\",
          dest=\"val_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = run_generate_features(**_parsed_args)\n\n_output_serializers
          = [\n    str,\n    str,\n    str,\n\n]\n\nimport os\nfor idx, output_file
          in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.io/in92/feature_generator:1.0.1"}}, "inputs": [{"name":
          "config", "type": "String"}, {"name": "mode", "type": "String"}, {"name":
          "bucket", "type": "String"}, {"name": "train_path", "type": "GCSPath"},
          {"name": "test_path", "type": "GCSPath"}, {"name": "val_path", "type": "GCSPath"}],
          "name": "Run generate features", "outputs": [{"name": "train", "type": "GCSPath"},
          {"name": "test", "type": "GCSPath"}, {"name": "val", "type": "GCSPath"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "80c708fa0b5fc07087b412447961782066c2659582dcb122226e886e43904019",
          "url": "../deliverables/components/feature_engineering/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"bucket": "{{inputs.parameters.bucket}}",
          "config": "{{inputs.parameters.config_file}}", "mode": "{{inputs.parameters.mode}}",
          "test_path": "{{inputs.parameters.run-prepare-test}}", "train_path": "{{inputs.parameters.run-prepare-train}}",
          "val_path": "{{inputs.parameters.run-prepare-val}}"}'}
  - name: run-prepare
    container:
      args: [--config, '{{inputs.parameters.config_file}}', --mode, '{{inputs.parameters.mode}}',
        --bucket, '{{inputs.parameters.bucket}}', --train-path, '{{inputs.parameters.collect-data-train}}',
        --test-path, '{{inputs.parameters.collect-data-test}}', --val-path, '{{inputs.parameters.collect-data-val}}',
        '----output-paths', /tmp/outputs/train/data, /tmp/outputs/test/data, /tmp/outputs/val/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def run_prepare(config,
                        mode,
                        bucket,
                        train_path,
                        test_path,
                        val_path):
            # Libraries --------------------------------------------------------------------------------------------------------
            import logging.config
            import yaml
            import sys
            import os
            from src.prepare import DataPreparer
            from src.helpers import load_data, save_data

            # Settings ---------------------------------------------------------------------------------------------------------
            logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',
                                datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)
            try:
                logging.info('Initializing configuration...')
                stream = open(config, 'r')
                config = yaml.load(stream=stream, Loader=yaml.FullLoader)
                preparer = DataPreparer(config=config)
                input_paths = [train_path, test_path, val_path]

                if mode == 'cloud':
                    output_paths_gcs = []
                    for input_path, out_filename in zip(input_paths, config['processed_data']):
                        data = load_data(input_data=input_path, mode=mode)
                        processed_data = preparer.transform(data=data)
                        # TODO: Add metadata in the pipeline
                        print(processed_data.head(5))
                        out_path_gcs = save_data(df=processed_data, path=config['processed_path'],
                                                 out_data=out_filename, mode=mode, bucket=bucket)
                        output_paths_gcs.append(out_path_gcs)
                    return tuple(output_paths_gcs)

                else:
                    output_paths = []
                    for input_filename, out_filename in zip(config['interim_data'], config['processed_data']):
                        data_path = os.path.join(config['interim_path'], input_filename)
                        data = load_data(input_data=data_path, mode=mode)
                        processed_data = preparer.transform(data=data)
                        # TODO: Add metadata in the pipeline
                        print(processed_data.head(5))
                        out_path = save_data(df=processed_data, path=config['processed_path'],
                                             out_data=out_filename, mode=mode, bucket=bucket)
                        output_paths.append(out_path)
                    return tuple(output_paths)

            except RuntimeError as error:
                logging.info(error)
                sys.exit(1)

        import argparse
        _parser = argparse.ArgumentParser(prog='Run prepare', description='')
        _parser.add_argument("--config", dest="config", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mode", dest="mode", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--bucket", dest="bucket", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train-path", dest="train_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-path", dest="test_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--val-path", dest="val_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = run_prepare(**_parsed_args)

        _output_serializers = [
            str,
            str,
            str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      env:
      - {name: GOOGLE_APPLICATION_CREDENTIALS, value: /secret/gcp-credentials/user-gcp-sa.json}
      - {name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE, value: /secret/gcp-credentials/user-gcp-sa.json}
      image: docker.io/in92/data_prepare:1.0.0
      volumeMounts:
      - {mountPath: /secret/gcp-credentials, name: gcp-credentials-user-gcp-sa}
    inputs:
      parameters:
      - {name: bucket}
      - {name: collect-data-test}
      - {name: collect-data-train}
      - {name: collect-data-val}
      - {name: config_file}
      - {name: mode}
    outputs:
      parameters:
      - name: run-prepare-test
        valueFrom: {path: /tmp/outputs/test/data}
      - name: run-prepare-train
        valueFrom: {path: /tmp/outputs/train/data}
      - name: run-prepare-val
        valueFrom: {path: /tmp/outputs/val/data}
      artifacts:
      - {name: run-prepare-test, path: /tmp/outputs/test/data}
      - {name: run-prepare-train, path: /tmp/outputs/train/data}
      - {name: run-prepare-val, path: /tmp/outputs/val/data}
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret: {secretName: user-gcp-sa}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--config", {"inputValue": "config"}, "--mode", {"inputValue":
          "mode"}, "--bucket", {"inputValue": "bucket"}, "--train-path", {"inputValue":
          "train_path"}, "--test-path", {"inputValue": "test_path"}, "--val-path",
          {"inputValue": "val_path"}, "----output-paths", {"outputPath": "train"},
          {"outputPath": "test"}, {"outputPath": "val"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def run_prepare(config,\n                mode,\n                bucket,\n                train_path,\n                test_path,\n                val_path):\n    #
          Libraries --------------------------------------------------------------------------------------------------------\n    import
          logging.config\n    import yaml\n    import sys\n    import os\n    from
          src.prepare import DataPreparer\n    from src.helpers import load_data,
          save_data\n\n    # Settings ---------------------------------------------------------------------------------------------------------\n    logging.basicConfig(format=''%(asctime)s
          %(levelname)s %(message)s'',\n                        datefmt=''%m/%d/%Y
          %I:%M:%S %p'', level=logging.INFO)\n    try:\n        logging.info(''Initializing
          configuration...'')\n        stream = open(config, ''r'')\n        config
          = yaml.load(stream=stream, Loader=yaml.FullLoader)\n        preparer = DataPreparer(config=config)\n        input_paths
          = [train_path, test_path, val_path]\n\n        if mode == ''cloud'':\n            output_paths_gcs
          = []\n            for input_path, out_filename in zip(input_paths, config[''processed_data'']):\n                data
          = load_data(input_data=input_path, mode=mode)\n                processed_data
          = preparer.transform(data=data)\n                # TODO: Add metadata in
          the pipeline\n                print(processed_data.head(5))\n                out_path_gcs
          = save_data(df=processed_data, path=config[''processed_path''],\n                                         out_data=out_filename,
          mode=mode, bucket=bucket)\n                output_paths_gcs.append(out_path_gcs)\n            return
          tuple(output_paths_gcs)\n\n        else:\n            output_paths = []\n            for
          input_filename, out_filename in zip(config[''interim_data''], config[''processed_data'']):\n                data_path
          = os.path.join(config[''interim_path''], input_filename)\n                data
          = load_data(input_data=data_path, mode=mode)\n                processed_data
          = preparer.transform(data=data)\n                # TODO: Add metadata in
          the pipeline\n                print(processed_data.head(5))\n                out_path
          = save_data(df=processed_data, path=config[''processed_path''],\n                                     out_data=out_filename,
          mode=mode, bucket=bucket)\n                output_paths.append(out_path)\n            return
          tuple(output_paths)\n\n    except RuntimeError as error:\n        logging.info(error)\n        sys.exit(1)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Run prepare'', description='''')\n_parser.add_argument(\"--config\",
          dest=\"config\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mode\",
          dest=\"mode\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--bucket\",
          dest=\"bucket\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-path\",
          dest=\"train_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-path\",
          dest=\"test_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--val-path\",
          dest=\"val_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = run_prepare(**_parsed_args)\n\n_output_serializers
          = [\n    str,\n    str,\n    str,\n\n]\n\nimport os\nfor idx, output_file
          in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.io/in92/data_prepare:1.0.0"}}, "inputs": [{"name": "config",
          "type": "String"}, {"name": "mode", "type": "String"}, {"name": "bucket",
          "type": "String"}, {"name": "train_path", "type": "GCSPath"}, {"name": "test_path",
          "type": "GCSPath"}, {"name": "val_path", "type": "GCSPath"}], "name": "Run
          prepare", "outputs": [{"name": "train", "type": "GCSPath"}, {"name": "test",
          "type": "GCSPath"}, {"name": "val", "type": "GCSPath"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "490305762ff2b2186ee71bbe6e53913ea201e7db04d430fd059b9f465adf6d4c", "url":
          "../deliverables/components/data_preparation/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"bucket":
          "{{inputs.parameters.bucket}}", "config": "{{inputs.parameters.config_file}}",
          "mode": "{{inputs.parameters.mode}}", "test_path": "{{inputs.parameters.collect-data-test}}",
          "train_path": "{{inputs.parameters.collect-data-train}}", "val_path": "{{inputs.parameters.collect-data-val}}"}'}
  - name: validate-data
    container:
      args: [--config, '{{inputs.parameters.config_file}}', --mode, '{{inputs.parameters.mode}}',
        --bucket, '{{inputs.parameters.bucket}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def run_validate(config,
                         mode,
                         bucket):

            # Libraries --------------------------------------------------------------------------------------------------------
            import logging
            import yaml
            from src.validate import DataValidator
            from src.helpers import load_data
            import sys

            # Settings ---------------------------------------------------------------------------------------------------------
            logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',
                                datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)

            try:
                logging.info('Initializing configuration...')
                stream = open(config, 'r')
                config = yaml.load(stream=stream, Loader=yaml.FullLoader)
                validator = DataValidator(config=config)
                df = load_data(input_path=config['raw_path'], input_data=config['raw_data'], mode=mode, bucket=bucket)
                validation_status = validator.validate(df=df)
                validator.check_validity(validation_status=validation_status)
            except RuntimeError as error:
                logging.info(error)
                sys.exit(1)

        import argparse
        _parser = argparse.ArgumentParser(prog='Run validate', description='')
        _parser.add_argument("--config", dest="config", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mode", dest="mode", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--bucket", dest="bucket", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = run_validate(**_parsed_args)
      env:
      - {name: GOOGLE_APPLICATION_CREDENTIALS, value: /secret/gcp-credentials/user-gcp-sa.json}
      - {name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE, value: /secret/gcp-credentials/user-gcp-sa.json}
      image: docker.io/in92/data_validate:latest
      volumeMounts:
      - {mountPath: /secret/gcp-credentials, name: gcp-credentials-user-gcp-sa}
    inputs:
      parameters:
      - {name: bucket}
      - {name: config_file}
      - {name: mode}
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret: {secretName: user-gcp-sa}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--config", {"inputValue": "config"}, "--mode", {"inputValue":
          "mode"}, "--bucket", {"inputValue": "bucket"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def run_validate(config,\n                 mode,\n                 bucket):\n\n    #
          Libraries --------------------------------------------------------------------------------------------------------\n    import
          logging\n    import yaml\n    from src.validate import DataValidator\n    from
          src.helpers import load_data\n    import sys\n\n    # Settings ---------------------------------------------------------------------------------------------------------\n    logging.basicConfig(format=''%(asctime)s
          %(levelname)s %(message)s'',\n                        datefmt=''%m/%d/%Y
          %I:%M:%S %p'', level=logging.INFO)\n\n    try:\n        logging.info(''Initializing
          configuration...'')\n        stream = open(config, ''r'')\n        config
          = yaml.load(stream=stream, Loader=yaml.FullLoader)\n        validator =
          DataValidator(config=config)\n        df = load_data(input_path=config[''raw_path''],
          input_data=config[''raw_data''], mode=mode, bucket=bucket)\n        validation_status
          = validator.validate(df=df)\n        validator.check_validity(validation_status=validation_status)\n    except
          RuntimeError as error:\n        logging.info(error)\n        sys.exit(1)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Run validate'', description='''')\n_parser.add_argument(\"--config\",
          dest=\"config\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mode\",
          dest=\"mode\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--bucket\",
          dest=\"bucket\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = run_validate(**_parsed_args)\n"],
          "image": "docker.io/in92/data_validate:latest"}}, "inputs": [{"name": "config",
          "type": "String"}, {"name": "mode", "type": "String"}, {"name": "bucket",
          "type": "String"}], "name": "Validate data"}', pipelines.kubeflow.org/component_ref: '{"digest":
          "d9e31106811fe7882bbd00699f0344df851932ef506c4c760bb45574e26aa4a6", "url":
          "../deliverables/components/data_validation/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"bucket":
          "{{inputs.parameters.bucket}}", "config": "{{inputs.parameters.config_file}}",
          "mode": "{{inputs.parameters.mode}}"}'}
  - name: women-clothing-reviews-classification-ml-pipeline
    inputs:
      parameters:
      - {name: bucket}
      - {name: config_file}
      - {name: mode}
    dag:
      tasks:
      - name: collect-data
        template: collect-data
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: config_file, value: '{{inputs.parameters.config_file}}'}
          - {name: mode, value: '{{inputs.parameters.mode}}'}
      - name: run-generate-features
        template: run-generate-features
        dependencies: [run-prepare]
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: config_file, value: '{{inputs.parameters.config_file}}'}
          - {name: mode, value: '{{inputs.parameters.mode}}'}
          - {name: run-prepare-test, value: '{{tasks.run-prepare.outputs.parameters.run-prepare-test}}'}
          - {name: run-prepare-train, value: '{{tasks.run-prepare.outputs.parameters.run-prepare-train}}'}
          - {name: run-prepare-val, value: '{{tasks.run-prepare.outputs.parameters.run-prepare-val}}'}
      - name: run-prepare
        template: run-prepare
        dependencies: [collect-data]
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: collect-data-test, value: '{{tasks.collect-data.outputs.parameters.collect-data-test}}'}
          - {name: collect-data-train, value: '{{tasks.collect-data.outputs.parameters.collect-data-train}}'}
          - {name: collect-data-val, value: '{{tasks.collect-data.outputs.parameters.collect-data-val}}'}
          - {name: config_file, value: '{{inputs.parameters.config_file}}'}
          - {name: mode, value: '{{inputs.parameters.mode}}'}
      - name: validate-data
        template: validate-data
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: config_file, value: '{{inputs.parameters.config_file}}'}
          - {name: mode, value: '{{inputs.parameters.mode}}'}
  arguments:
    parameters:
    - {name: config_file, value: config.yaml}
    - {name: mode}
    - {name: bucket}
  serviceAccountName: pipeline-runner

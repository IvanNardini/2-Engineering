apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: women-clothing-reviews-classification-ml-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.4.0, pipelines.kubeflow.org/pipeline_compilation_time: '2021-03-18T07:16:50.544589',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "An example of Machine
      Learning Pipeline", "inputs": [{"default": "config.yaml", "name": "config_file",
      "optional": true}, {"name": "mode", "optional": true}, {"name": "bucket", "optional":
      true}], "name": "Women Clothing Reviews Classification ML Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.4.0}
spec:
  entrypoint: women-clothing-reviews-classification-ml-pipeline
  templates:
  - name: collect-data
    container:
      args: [--config, '{{inputs.parameters.config_file}}', --mode, '{{inputs.parameters.mode}}',
        --bucket, '{{inputs.parameters.bucket}}', '----output-paths', /tmp/outputs/train/data,
        /tmp/outputs/test/data, /tmp/outputs/val/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def run_collect(config,
                        mode,
                        bucket):

            # Libraries --------------------------------------------------------------------------------------------------------
            import logging
            import yaml
            from collections import namedtuple
            import sys
            from src.collect import DataCollector

            # Settings ---------------------------------------------------------------------------------------------------------
            logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',
                                datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)

            logging.info('Initializing pipeline configuration...')

            try:
                stream = open(config, 'r')
                config = yaml.load(stream=stream, Loader=yaml.FullLoader)
                collector = DataCollector(config=config)
                raw_df = collector.extract()
                # TODO: Add metadata in the pipeline
                print(raw_df.head(5))
                x_train, x_test, x_val, y_train, y_test, y_val = collector.transform(raw_df)

                if mode == 'cloud':
                    (train_path_gcs, test_path_gcs, val_path_gcs) = collector.load(x_train, x_test, x_val,
                                                                                   y_train, y_test, y_val, mode=mode,
                                                                                   bucket=bucket)
                    out_gcs = namedtuple('output_paths', ['train', 'test', 'val'])
                    return out_gcs(train_path_gcs, test_path_gcs, val_path_gcs)
                else:
                    (train_path, test_path, val_path) = collector.load(x_train, x_test, x_val,
                                                                       y_train, y_test, y_val, mode=mode, bucket=bucket)
                    out_path = namedtuple('output_paths', ['train', 'test', 'val'])
                    return out_path(train_path, test_path, val_path)
            except RuntimeError as error:
                logging.info(error)
                sys.exit(1)

        import argparse
        _parser = argparse.ArgumentParser(prog='Run collect', description='')
        _parser.add_argument("--config", dest="config", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mode", dest="mode", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--bucket", dest="bucket", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = run_collect(**_parsed_args)

        _output_serializers = [
            str,
            str,
            str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      env:
      - {name: GOOGLE_APPLICATION_CREDENTIALS, value: /secret/gcp-credentials/user-gcp-sa.json}
      - {name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE, value: /secret/gcp-credentials/user-gcp-sa.json}
      image: docker.io/in92/data_collect:latest
      volumeMounts:
      - {mountPath: /secret/gcp-credentials, name: gcp-credentials-user-gcp-sa}
    inputs:
      parameters:
      - {name: bucket}
      - {name: config_file}
      - {name: mode}
    outputs:
      parameters:
      - name: collect-data-test
        valueFrom: {path: /tmp/outputs/test/data}
      - name: collect-data-train
        valueFrom: {path: /tmp/outputs/train/data}
      - name: collect-data-val
        valueFrom: {path: /tmp/outputs/val/data}
      artifacts:
      - {name: collect-data-test, path: /tmp/outputs/test/data}
      - {name: collect-data-train, path: /tmp/outputs/train/data}
      - {name: collect-data-val, path: /tmp/outputs/val/data}
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret: {secretName: user-gcp-sa}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--config", {"inputValue": "config"}, "--mode", {"inputValue":
          "mode"}, "--bucket", {"inputValue": "bucket"}, "----output-paths", {"outputPath":
          "train"}, {"outputPath": "test"}, {"outputPath": "val"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def run_collect(config,\n                mode,\n                bucket):\n\n    #
          Libraries --------------------------------------------------------------------------------------------------------\n    import
          logging\n    import yaml\n    from collections import namedtuple\n    import
          sys\n    from src.collect import DataCollector\n\n    # Settings ---------------------------------------------------------------------------------------------------------\n    logging.basicConfig(format=''%(asctime)s
          %(levelname)s %(message)s'',\n                        datefmt=''%m/%d/%Y
          %I:%M:%S %p'', level=logging.INFO)\n\n    logging.info(''Initializing pipeline
          configuration...'')\n\n    try:\n        stream = open(config, ''r'')\n        config
          = yaml.load(stream=stream, Loader=yaml.FullLoader)\n        collector =
          DataCollector(config=config)\n        raw_df = collector.extract()\n        #
          TODO: Add metadata in the pipeline\n        print(raw_df.head(5))\n        x_train,
          x_test, x_val, y_train, y_test, y_val = collector.transform(raw_df)\n\n        if
          mode == ''cloud'':\n            (train_path_gcs, test_path_gcs, val_path_gcs)
          = collector.load(x_train, x_test, x_val,\n                                                                           y_train,
          y_test, y_val, mode=mode,\n                                                                           bucket=bucket)\n            out_gcs
          = namedtuple(''output_paths'', [''train'', ''test'', ''val''])\n            return
          out_gcs(train_path_gcs, test_path_gcs, val_path_gcs)\n        else:\n            (train_path,
          test_path, val_path) = collector.load(x_train, x_test, x_val,\n                                                               y_train,
          y_test, y_val, mode=mode, bucket=bucket)\n            out_path = namedtuple(''output_paths'',
          [''train'', ''test'', ''val''])\n            return out_path(train_path,
          test_path, val_path)\n    except RuntimeError as error:\n        logging.info(error)\n        sys.exit(1)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Run collect'', description='''')\n_parser.add_argument(\"--config\",
          dest=\"config\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mode\",
          dest=\"mode\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--bucket\",
          dest=\"bucket\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = run_collect(**_parsed_args)\n\n_output_serializers
          = [\n    str,\n    str,\n    str,\n\n]\n\nimport os\nfor idx, output_file
          in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.io/in92/data_collect:latest"}}, "inputs": [{"name": "config",
          "type": "String"}, {"name": "mode", "type": "String"}, {"name": "bucket",
          "type": "String"}], "name": "Collect data", "outputs": [{"name": "train",
          "type": "GCSPath"}, {"name": "test", "type": "GCSPath"}, {"name": "val",
          "type": "GCSPath"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "dc3bf4eb447156407209361299a5d1f3fad03e300a73518a88e8a58b1ec81e94", "url":
          "../deliverables/components/data_collection/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"bucket":
          "{{inputs.parameters.bucket}}", "config": "{{inputs.parameters.config_file}}",
          "mode": "{{inputs.parameters.mode}}"}'}
  - name: prepare-data
    container:
      args: [--config, '{{inputs.parameters.config_file}}', --mode, '{{inputs.parameters.mode}}',
        --bucket, '{{inputs.parameters.bucket}}', --train-path, '{{inputs.parameters.collect-data-train}}',
        --test-path, '{{inputs.parameters.collect-data-test}}', --val-path, '{{inputs.parameters.collect-data-val}}',
        '----output-paths', /tmp/outputs/train/data, /tmp/outputs/test/data, /tmp/outputs/val/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def run_prepare(config,
                        mode,
                        bucket,
                        train_path,
                        test_path,
                        val_path):
            # Libraries --------------------------------------------------------------------------------------------------------
            import logging.config
            import yaml
            import sys
            import os
            from src.prepare import DataPreparer
            from src.helpers import load_data, save_data

            # Settings ---------------------------------------------------------------------------------------------------------
            logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',
                                datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)

            try:
                stream = open(config, 'r')
                config = yaml.load(stream=stream, Loader=yaml.FullLoader)
                preparer = DataPreparer(config=config)
                input_paths = [train_path, test_path, val_path]

                if mode == 'cloud':
                    output_paths_gcs = []
                    for input_path, filename in zip(input_paths, config['processed_data']):
                        data = load_data(input_data=input_path, mode=mode)
                        processed_data = preparer.transform(data=data)
                        # TODO: Add metadata in the pipeline
                        print(processed_data.head(5))
                        out_path_gcs = save_data(df=processed_data, path=config['processed_path'],
                                                 out_data=filename, mode=mode, bucket=bucket)
                        output_paths_gcs.append(out_path_gcs)
                    return tuple(output_paths_gcs)

                else:
                    output_paths = []
                    for input_filename, out_filename in zip(config['interim_data'], config['processed_data']):
                        data_path = os.path.join(config['interim_path'], input_filename)
                        data = load_data(input_data=data_path, mode=mode)
                        processed_data = preparer.transform(data=data)
                        # TODO: Add metadata in the pipeline
                        print(processed_data.head(5))
                        out_path = save_data(df=processed_data, path=config['processed_path'],
                                             out_data=out_filename, mode=mode, bucket=bucket)
                        output_paths.append(out_path)
                    return tuple(output_paths)

            except RuntimeError as error:
                logging.info(error)
                sys.exit(1)

        import argparse
        _parser = argparse.ArgumentParser(prog='Run prepare', description='')
        _parser.add_argument("--config", dest="config", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mode", dest="mode", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--bucket", dest="bucket", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train-path", dest="train_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-path", dest="test_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--val-path", dest="val_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = run_prepare(**_parsed_args)

        _output_serializers = [
            str,
            str,
            str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      env:
      - {name: GOOGLE_APPLICATION_CREDENTIALS, value: /secret/gcp-credentials/user-gcp-sa.json}
      - {name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE, value: /secret/gcp-credentials/user-gcp-sa.json}
      image: docker.io/in92/data_prepare:latest
      volumeMounts:
      - {mountPath: /secret/gcp-credentials, name: gcp-credentials-user-gcp-sa}
    inputs:
      parameters:
      - {name: bucket}
      - {name: collect-data-test}
      - {name: collect-data-train}
      - {name: collect-data-val}
      - {name: config_file}
      - {name: mode}
    outputs:
      artifacts:
      - {name: prepare-data-test, path: /tmp/outputs/test/data}
      - {name: prepare-data-train, path: /tmp/outputs/train/data}
      - {name: prepare-data-val, path: /tmp/outputs/val/data}
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret: {secretName: user-gcp-sa}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--config", {"inputValue": "config"}, "--mode", {"inputValue":
          "mode"}, "--bucket", {"inputValue": "bucket"}, "--train-path", {"inputValue":
          "train_path"}, "--test-path", {"inputValue": "test_path"}, "--val-path",
          {"inputValue": "val_path"}, "----output-paths", {"outputPath": "train"},
          {"outputPath": "test"}, {"outputPath": "val"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def run_prepare(config,\n                mode,\n                bucket,\n                train_path,\n                test_path,\n                val_path):\n    #
          Libraries --------------------------------------------------------------------------------------------------------\n    import
          logging.config\n    import yaml\n    import sys\n    import os\n    from
          src.prepare import DataPreparer\n    from src.helpers import load_data,
          save_data\n\n    # Settings ---------------------------------------------------------------------------------------------------------\n    logging.basicConfig(format=''%(asctime)s
          %(levelname)s %(message)s'',\n                        datefmt=''%m/%d/%Y
          %I:%M:%S %p'', level=logging.INFO)\n\n    try:\n        stream = open(config,
          ''r'')\n        config = yaml.load(stream=stream, Loader=yaml.FullLoader)\n        preparer
          = DataPreparer(config=config)\n        input_paths = [train_path, test_path,
          val_path]\n\n        if mode == ''cloud'':\n            output_paths_gcs
          = []\n            for input_path, filename in zip(input_paths, config[''processed_data'']):\n                data
          = load_data(input_data=input_path, mode=mode)\n                processed_data
          = preparer.transform(data=data)\n                # TODO: Add metadata in
          the pipeline\n                print(processed_data.head(5))\n                out_path_gcs
          = save_data(df=processed_data, path=config[''processed_path''],\n                                         out_data=filename,
          mode=mode, bucket=bucket)\n                output_paths_gcs.append(out_path_gcs)\n            return
          tuple(output_paths_gcs)\n\n        else:\n            output_paths = []\n            for
          input_filename, out_filename in zip(config[''interim_data''], config[''processed_data'']):\n                data_path
          = os.path.join(config[''interim_path''], input_filename)\n                data
          = load_data(input_data=data_path, mode=mode)\n                processed_data
          = preparer.transform(data=data)\n                # TODO: Add metadata in
          the pipeline\n                print(processed_data.head(5))\n                out_path
          = save_data(df=processed_data, path=config[''processed_path''],\n                                     out_data=out_filename,
          mode=mode, bucket=bucket)\n                output_paths.append(out_path)\n            return
          tuple(output_paths)\n\n    except RuntimeError as error:\n        logging.info(error)\n        sys.exit(1)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Run prepare'', description='''')\n_parser.add_argument(\"--config\",
          dest=\"config\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mode\",
          dest=\"mode\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--bucket\",
          dest=\"bucket\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-path\",
          dest=\"train_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-path\",
          dest=\"test_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--val-path\",
          dest=\"val_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = run_prepare(**_parsed_args)\n\n_output_serializers
          = [\n    str,\n    str,\n    str,\n\n]\n\nimport os\nfor idx, output_file
          in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.io/in92/data_prepare:latest"}}, "inputs": [{"name": "config",
          "type": "String"}, {"name": "mode", "type": "String"}, {"name": "bucket",
          "type": "String"}, {"name": "train_path", "type": "GCSPath"}, {"name": "test_path",
          "type": "GCSPath"}, {"name": "val_path", "type": "GCSPath"}], "name": "Prepare
          data", "outputs": [{"name": "train", "type": "GCSPath"}, {"name": "test",
          "type": "GCSPath"}, {"name": "val", "type": "GCSPath"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "0430bb620009fd01e5c14e45d23b29bb79f31c82eb79176160a6fee23b305b6b", "url":
          "../deliverables/components/data_preparation/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"bucket":
          "{{inputs.parameters.bucket}}", "config": "{{inputs.parameters.config_file}}",
          "mode": "{{inputs.parameters.mode}}", "test_path": "{{inputs.parameters.collect-data-test}}",
          "train_path": "{{inputs.parameters.collect-data-train}}", "val_path": "{{inputs.parameters.collect-data-val}}"}'}
  - name: women-clothing-reviews-classification-ml-pipeline
    inputs:
      parameters:
      - {name: bucket}
      - {name: config_file}
      - {name: mode}
    dag:
      tasks:
      - name: collect-data
        template: collect-data
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: config_file, value: '{{inputs.parameters.config_file}}'}
          - {name: mode, value: '{{inputs.parameters.mode}}'}
      - name: prepare-data
        template: prepare-data
        dependencies: [collect-data]
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: collect-data-test, value: '{{tasks.collect-data.outputs.parameters.collect-data-test}}'}
          - {name: collect-data-train, value: '{{tasks.collect-data.outputs.parameters.collect-data-train}}'}
          - {name: collect-data-val, value: '{{tasks.collect-data.outputs.parameters.collect-data-val}}'}
          - {name: config_file, value: '{{inputs.parameters.config_file}}'}
          - {name: mode, value: '{{inputs.parameters.mode}}'}
  arguments:
    parameters:
    - {name: config_file, value: config.yaml}
    - {name: mode}
    - {name: bucket}
  serviceAccountName: pipeline-runner

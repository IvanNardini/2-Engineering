apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: women-clothing-reviews-classification-ml-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.4.0, pipelines.kubeflow.org/pipeline_compilation_time: '2021-03-10T10:03:02.668594',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "An example of Machine
      Learning Pipeline", "inputs": [{"name": "config", "type": "URI"}, {"name": "mode",
      "type": "PipelineParam"}], "name": "Women Clothing Reviews Classification ML
      Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.4.0}
spec:
  entrypoint: women-clothing-reviews-classification-ml-pipeline
  templates:
  - name: collect-data
    container:
      args: [--config, '{{inputs.parameters.read-pipeline-configuration-Data}}', --mode,
        '{{inputs.parameters.mode}}']
      env:
      - {name: GOOGLE_APPLICATION_CREDENTIALS, value: /secret/gcp-credentials/user-gcp-sa.json}
      - {name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE, value: /secret/gcp-credentials/user-gcp-sa.json}
      image: docker.io/in92/data_collect:1.0.0
      volumeMounts:
      - {mountPath: /secret/gcp-credentials, name: gcp-credentials-user-gcp-sa}
    inputs:
      parameters:
      - {name: mode}
      - {name: read-pipeline-configuration-Data}
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret: {secretName: user-gcp-sa}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"inputs": [{"name": "config"},
          {"name": "mode"}], "name": "Data collection"}'}
  - name: pipeline-volume
    resource:
      action: create
      manifest: |
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: '{{workflow.name}}-data'
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 3Gi
    outputs:
      parameters:
      - name: pipeline-volume-manifest
        valueFrom: {jsonPath: '{}'}
      - name: pipeline-volume-name
        valueFrom: {jsonPath: '{.metadata.name}'}
      - name: pipeline-volume-size
        valueFrom: {jsonPath: '{.status.capacity.storage}'}
  - name: read-pipeline-configuration
    container:
      args: []
      command:
      - bash
      - -ex
      - -c
      - |
        if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
            gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
        fi

        uri="$0"
        output_path="$1"

        # Checking whether the URI points to a single blob, a directory or a URI pattern
        # URI points to a blob when that URI does not end with slash and listing that URI only yields the same URI
        if [[ "$uri" != */ ]] && (gsutil ls "$uri" | grep --fixed-strings --line-regexp "$uri"); then
            mkdir -p "$(dirname "$output_path")"
            gsutil -m cp -r "$uri" "$output_path"
        else
            mkdir -p "$output_path" # When source path is a directory, gsutil requires the destination to also be a directory
            gsutil -m rsync -r "$uri" "$output_path" # gsutil cp has different path handling than Linux cp. It always puts the source directory (name) inside the destination directory. gsutil rsync does not have that problem.
        fi
      - '{{inputs.parameters.config}}'
      - /tmp/outputs/Data/data
      image: google/cloud-sdk
      volumeMounts:
      - {mountPath: /pipe-data, name: pipeline-volume}
    inputs:
      parameters:
      - {name: config}
      - {name: pipeline-volume-name}
    outputs:
      parameters:
      - name: read-pipeline-configuration-Data
        valueFrom: {path: /tmp/outputs/Data/data}
      artifacts:
      - {name: read-pipeline-configuration-Data, path: /tmp/outputs/Data/data}
    volumes:
    - name: pipeline-volume
      persistentVolumeClaim: {claimName: '{{inputs.parameters.pipeline-volume-name}}'}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"command": ["bash", "-ex", "-c", "if [ -n \"${GOOGLE_APPLICATION_CREDENTIALS}\"
          ]; then\n    gcloud auth activate-service-account --key-file=\"${GOOGLE_APPLICATION_CREDENTIALS}\"\nfi\n\nuri=\"$0\"\noutput_path=\"$1\"\n\n#
          Checking whether the URI points to a single blob, a directory or a URI pattern\n#
          URI points to a blob when that URI does not end with slash and listing that
          URI only yields the same URI\nif [[ \"$uri\" != */ ]] && (gsutil ls \"$uri\"
          | grep --fixed-strings --line-regexp \"$uri\"); then\n    mkdir -p \"$(dirname
          \"$output_path\")\"\n    gsutil -m cp -r \"$uri\" \"$output_path\"\nelse\n    mkdir
          -p \"$output_path\" # When source path is a directory, gsutil requires the
          destination to also be a directory\n    gsutil -m rsync -r \"$uri\" \"$output_path\"
          # gsutil cp has different path handling than Linux cp. It always puts the
          source directory (name) inside the destination directory. gsutil rsync does
          not have that problem.\nfi\n", {"inputValue": "GCS path"}, {"outputPath":
          "Data"}], "image": "google/cloud-sdk"}}, "inputs": [{"name": "GCS path",
          "type": "URI"}], "name": "Read Pipeline configuration", "outputs": [{"name":
          "Data"}]}', pipelines.kubeflow.org/component_ref: '{"digest": "1fbe46e98817e5bc74b7c1e3ebe57b5dd57403d74b5123170aa270b95a0471ef",
          "url": "components/config_init/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"GCS
          path": "{{inputs.parameters.config}}"}'}
  - name: women-clothing-reviews-classification-ml-pipeline
    inputs:
      parameters:
      - {name: config}
      - {name: mode}
    dag:
      tasks:
      - name: collect-data
        template: collect-data
        dependencies: [read-pipeline-configuration]
        arguments:
          parameters:
          - {name: mode, value: '{{inputs.parameters.mode}}'}
          - {name: read-pipeline-configuration-Data, value: '{{tasks.read-pipeline-configuration.outputs.parameters.read-pipeline-configuration-Data}}'}
      - {name: pipeline-volume, template: pipeline-volume}
      - name: read-pipeline-configuration
        template: read-pipeline-configuration
        dependencies: [pipeline-volume]
        arguments:
          parameters:
          - {name: config, value: '{{inputs.parameters.config}}'}
          - {name: pipeline-volume-name, value: '{{tasks.pipeline-volume.outputs.parameters.pipeline-volume-name}}'}
  arguments:
    parameters:
    - {name: config}
    - {name: mode}
  serviceAccountName: pipeline-runner
